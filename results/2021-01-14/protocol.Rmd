---
title: "Double-digest restriction-associated DNA sequencing of *Coregonus lavaretus*"
author: "J. Ignacio Lucas Lledó"
date: "14/1/2021"
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project size
We attempt to prepare over 600 DNA libraries for ddRAD-seq. We plan for 3 NextSeq sequencing runs, expecting each to yield about 300 million 150-bases, single-end reads. Assuming successful sequencing, and well balanced among samples, that would produce around 1.5 million reads per sample. But that would produce a very modest mean coverage of $\sim$15 per locus if we were sequencing around 100000 loci. It is, therefore essential to limit the number of loci to not much more than 50000. Such a modest number of loci should be enough for the purpose of population clustering and phylogenetics. A tight limit in the number of loci should also help optimize the fraction of loci that get well covered in all or most of the samples: the probability of a locus getting sequenced deep enough in all samples is $p^N$, $p$ being the probability of the locus getting sequenced deep enough in one sample, and $N$, the number of samples. And in a best-case scenario, $p$ is the Poisson probability of a locus getting sequenced at least a minimum number of times given the expected average coverage. Happily assuming all loci have the same chance of being sequenced, the graph below should show the optimistically expected fraction of 600 samples getting sequenced with a minimum coverage $k$, given an assumed average coverage.

```{r coverage}
MinimumCoverage <- 1:15
AverageCoverage <- c(10, 15, 20, 25)
FractionCovered <- lapply(AverageCoverage,
                          function(x) (1 - ppois(MinimumCoverage, x-1))^753)
# ppois(x,y) is P(X <= x). I subtract 1 to x to make threshold inclusive.
plot(c(0,15), c(0,1), type = 'n', xlab = 'Minimum coverage',
     ylab='Fraction of samples with minimum coverage')
lines(MinimumCoverage, FractionCovered[[1]], lty = 1)
lines(MinimumCoverage, FractionCovered[[2]], lty = 2)
lines(MinimumCoverage, FractionCovered[[3]], lty = 3)
lines(MinimumCoverage, FractionCovered[[4]], lty = 4)
legend(11, 0.95, legend = as.character(AverageCoverage),
       lty = 1:4, title = 'Average coverage')
```

Adding a third sequencing run could increase the expected average coverage to between 15 and 30, depending on the number of loci targeted (between 50.000 and 100.000). We will probably need it.

# State of the art
I have not found studies using double digest RAD sequencing in European whitefish. @Feulner2019 used traditional RAD sequencing with enzyme *SbfI*, random shearing and selection of sizes between 300 and 500 bases. They sequenced 500 million reads from 180 samples, identified 125154 loci and retained only 16173, in 138 individuals, after filtering. @De-Kayne2018 also use the *SbfI* enzyme. And the same enzyme is regularly used for digestion of American whitefish [@Gagnaire2013].

Mar did find a couple references that apply ddRAD-seq to \emph{Coregonus} species. @Recknagel2015 develop a ddRAD-seq protocol for Ion Torrent sequencing technologies. They digest the fish DNA with *PstI* (rare cutter) and *MspI* (frequent cutter), and select for two different ranges: 250-320 and 130-200 bases. They assemble 538133 stacks from the longer library and 378077 from the short one, although only a fraction of them are *shared* (I guess across libraries): 102959 and 87908 loci in the long and short libraries, respectively. I take these to mean that individual libraries waste a substantial sequencing depth in fragments that are likely outside the targeted size range, randomly included in any library due to imprecision in size selection. 

```{r digestioRecknagel2015}
suppressMessages(library(SimRAD))
if (file.exists('libLen.RData')) {
  load('libLen.RData')
} else {
  REFERENCE <- '../../data/reference.fa'
  refseq <- ref.DNAseq(REFERENCE, subselect.contigs = FALSE)
  # Restriction Enzyme 1 PstI
  cs_5p1 <- "CTGCA"
  cs_3p1 <- "G"
  # Restriction Enzyme 2 MspI
  cs_5p2 <- "C"
  cs_3p2 <- "CGG"
  simseq.dig <- insilico.digest(refseq, cs_5p1, cs_3p1, cs_5p2, cs_3p2, verbose=FALSE)
  simseq.sel <- adapt.select(simseq.dig, type="AB+BA", cs_5p1, cs_3p1, cs_5p2, cs_3p2)
  simseq.short <- size.select(simseq.sel, min.size=130, max.size=200,
                              graph=FALSE, verbose=FALSE)
  simseq.long  <- size.select(simseq.sel, min.size=250, max.size=320,
                              graph=FALSE, verbose=FALSE)
  simseq.crotti <- size.select(simseq.sel, min.size=150, max.size=300,
                               graph=FALSE, verbose=FALSE)
  simseq.max <- size.select(simseq.sel, min.size=700, max.size=850,
                            graph=FALSE, verbose=FALSE)
  LibLen <- data.frame(RecknagelShort = length(simseq.short),
                       RecknagelLong  = length(simseq.long),
                       Crotti = length(simseq.crotti),
                       Maximum = length(simseq.max))
  save(LibLen, file = 'libLen.RData')
}
```

Taking only shared loci into account, the number of loci identified somewhat resembles the in silico prediction: `r LibLen$RecknagelLong` and `r LibLen$RecknagelShort` in long and short ranges, respectively. The almost double number of longer fragments sequenced than expected is a mistery.

The other article is by @Crotti2020. They compare ddRAD-seq to EpiRAD-seq. For ddRAD-seq, they use the exact same combination of enzymes, *PstI* and *MspI*. They select 150-300 bp fragments and found 355491 loci among 43 individuals, but only 108127 loci per individual on average. They don't say the number of loci common to at least a few samples. The numbers are compatible with the in silico prediction of `r LibLen$Crotti` loci in the targeted size range, a random fraction of them being sequenced in any single individual.

Using frequent cutters, that generate an excess of loci, is not incompatible with selecting a small number of loci, if selected sizes are long enough, because cutting frequently produce mostly very short fragments and much fewer long ones. For example, in silico digestion with *PstI* and *MspI* suggests there may be `r LibLen$Maximum` fragments between 700 and 850 bp.

However, size selection is imprecise, many reads potentially being wasted in sequencing fragments from outside the range, which are not expected to be shared among libraries. How does the fragment length distribution affect the proportion of out-of-range fragments included in a library? It seems to me that the longer the range, the lower the proportion of out-of-range fragments should be retained, but it's difficult to tell. By using two rare cutters instead of one rare and one frequent cutter, the fragment length distribution is shifted to the right. A larger fraction of the genome will be left in big chunks. That could also afect (improve?) the efficiency of the ligation step, because it reduces the molarity of not-targeted, sticky DNA ends.

@DaCosta2014 (page 9) do mention that using a broad size range and rare cutters can produce excellent results, even using a simple gel, instead of a Pippin Prep, for size selection. They warn of very small fragments being carried over in the gel, and eventually producing valid data. The expected bias towards shorter fragments due to PCR amplification was not too bad. 

In conclusion, I think it's worth using two rare cutters, which allow for the selection of a wide range of fragment sizes without admitting an excessive number of fragments. See the report in `2020-12-14` for additional in silico digestions that help choose the restriction enzymes.

# Choice of restriction enzyme combination
When limiting the expected number of targeted fragments between 50000 and 65000, the combination with more unique fragments is *NsiI* and *SphI*. However, I am afraid that their products could spuriously cross-hybridize through the \textsf{CA}-3' and \textsf{TG}-3' ends of their respective overhangs. Using T7 DNA ligase, instead of T4, would increase the specificity for well paired substrate (see [NEB](https://international.neb.com/tools-and-resources/selection-charts/properties-of-dna-and-rna-ligases)). But T7 requires PEG in addition to ATP, and I don't know how PEG could affect the digestion. To simplify, we will use T4 DNA ligase.

The next best combinations are *SphI* with *HindIII* ($\sim59000$ expected targeted fragments) and *BmtI* with *BamHI* ($\sim 61000 expected targeted fragments). For the moment I choose *SphI* and *HindIII*, which share a recomended diluent buffer. See the document `adapters.pdf` for the adapters design.

```{r digestioSphI.HindIII}
if (file.exists('ends.RData')) {
  load('ends.RData')
} else {
  # Restriction Enzyme 1 SphI
  cs_5p1 <- "GCATG"
  cs_3p1 <- "C"
  # Restriction Enzyme 2 HindIII
  cs_5p2 <- "A"
  cs_3p2 <- "AGCTT"
  simseq.dig <- insilico.digest(refseq, cs_5p1, cs_3p1, cs_5p2, cs_3p2, verbose=FALSE)
  simseq.AB <- adapt.select(simseq.dig, type="AB+BA", cs_5p1, cs_3p1, cs_5p2, cs_3p2)
  simseq.AA <- adapt.select(simseq.dig, type="AA", cs_5p1, cs_3p1, cs_5p2, cs_3p2)
  simseq.BB <- adapt.select(simseq.dig, type="BB", cs_5p1, cs_3p1, cs_5p2, cs_3p2)
  A_ends <- length(simseq.AB) + 2 * length(simseq.AA)
  B_ends <- length(simseq.AB) + 2 * length(simseq.BB)
  ReferenceLength <- nchar(refseq) * 1.0e-6  # in megabases
  save(A_ends, B_ends, ReferenceLength, file = 'ends.RData')
}
```

If digestion was complete with both enzymes, and the reference genome was accurate, every genome copy would generate `r sprintf("%i", A_ends)` *SphI* ends and `r sprintf("%i", B_ends)` HindIII ends. However, the reference genome is 2100 Mb long, while all *Coregonus* species in the [Animal Genome Size Database](http://www.genomesize.com) have larger genomes, roughly estimated to be between 2400 and 3500 Mb. To account for a potential underestimate of the number of fragments targeted, we can narrow their size range down a little.

# Overview of the protocol
Amparo (SCSIE) suggested to use an optimized ddRAD-seq protocol that uses Illumina indices instead of inline barcodes [@Salas-Lizana2018]. That way, it is cheaper tu multiplex when using several samples. Briefly, the protocol includes the following steps:

- DNA extraction.
- Annealing of non-barcoded adapters (P1 and P2).
- Double digestion and simultaneous ligation of adapters.
- Clean up with magnetic beads and removing small fragments.
- PCR amplification and incorporation of Nextera Indices.
- Pooling samples in equimolar proportion.
- Size selection.

We will use a DNA extraction method different from the one suggested by @Salas-Lizana2018, and adequate for fish tissue, but respecting the recommendation of final concentration and volume. Annother difference is the simultaneous digestion and ligation. This simplification of two reactions in one step will be attempted after designing the universal adapters in a way that does not reproduce the recognition sites. According to @Salas-Lizana2018, the adapter specific of the *MseI* enzyme should be 10 times more concentrated than the adapter for *EcoRI* sites. I assume the reason is that *MseI* cuts more frequently.

The fourth step cleans up the digestion/ligation reaction while imposing a size threshold only on the left side of the distribution. The actual size selection step is supposed to happen after pooling the samples, at the end of the protocol. This seems to serve the purpose of minimizing the number of size-selection procedures, which could otherwise introduce variation among samples. I guess the fact of running the PCR amplification before removing larger DNA fragments is not concerning, because after all the targetted fragments being shorter, they are expected to amplify more efficiently.

# DNA extraction
Some protocols suggest a CTAB method of DNA extraction from plants and fungi [@Salas-Lizana2018; @Mastretta-Yanes2015]. Some studies on fish species use a Qiagen DNeasy kit for tissue or blood [@Feulner2019; @De-Kayne2018]. We'll use that. According to Qiagen, 20 mg of fish fin should yield 10--20 $\mu$g of DNA in an elution volume of 100-200 $\mu$l, which corresponds to at least `r 10 * 1000 / 200` ng/$\mu$l. The minimum starting DNA material required for ddRAD-seq is precisely 50 ng/$\mu$l [@Salas-Lizana2018].

According to the [Animal Genome Size Database](http://www.genomesize.com), *Coregonus* genomes have a haploid size between 2.44 and 3.54 pg, which would approximately correspond with between 2400 and 3500 Mb. The reference genome is less than 2100 Mb, which warns me about an important underestimation of the number of restriction fragments that we will obtain. For the purpose of estimating the number of fragment ends per $\mu$g of DNA, I don't need to correct for this underestimation, as long as I set the genome size equal to that of the reference genome. 

```{r mols}
MegabasesPerPicogram <- 978 # see Animal Genome Size Database
ReferenceWeight <- ReferenceLength / MegabasesPerPicogram # pg
CopiesPerMicrogram <- 1.0e+6 / ReferenceWeight
AvogadroNum <- 6.022141e+23
PicomolEndsPerMicrogram <- 10^12 * CopiesPerMicrogram * c(A_ends, B_ends) / AvogadroNum
names(PicomolEndsPerMicrogram) <- c('SphI', 'HindIII')
```

Thus, there could be up to `r round(PicomolEndsPerMicrogram['SphI'], 3)` pmol of *SphI* fragment ends and `r round(PicomolEndsPerMicrogram['HindIII'], 3)` pmol of *HindIII* fragment ends per $\mu$g of DNA. 

Alan started extracting DNA with Nucleospin. The yields are between 2760 and 26400 ng per sample, which are much
higher than I expected. Assuming a 3.54 pg per haploid genome, such a range suggests the number of diploid cells
contributing DNA to the sample could be between 0.39 and 3.7 millions, which I have a very hard time believing,
after having seen how small the tissue sample is. In any case, there seems to be plenty of DNA and our Qubit estimates
of concentration are the most reliable. (Nanodrop suggests even higher values). It is reasonable to balance the
samples before digestion. I will assume the DNA amount per reaction is 1 $\mu$g, which is about half the minimum observed
up to now.

# Annealing of adapters
We need to order the following sequences, for the adapters to match the overhangs produced by *HindIII* and *SphI*:

 Oligo                  Sequence
 ----------------       ----------
 *HindIII*-P1.1         `*AGCTCTGTCTCTTATACGAGAACAA`
 *HindIII*-P1.2         `GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG`
 *SphI*-P2.1            `GTCGGCAGCGTCAGATGTGTATAAGAGACAGCCATG`
 *SphI*-P2.2            `*GCTGTCTCTTATACACATCTGACGCTGCCGACGA`
 
Asteriscs indicate 5\'-phosphorylation. The maximum number of samples is 753. 

The oligos may be delivered dry, and need to be suspended in the appropriate volume of either water of TE buffer to reach a comfortable concentration, such as 100 $\mu$M (i.e., 100 pmol/$\mu$l). The annealing buffer stock (10$\times$) may be 100 mM Tris HCl (pH 8), 500 mM NaCl, 1 mM EDTA (see `ddRADprotocol.pdf`).

After annealing the adapters, I guess is a good idea to keep them cool.

# Double digestion and simultaneous ligation of adapters
We plan to use both enzymes at a concentration of 20000 U/ml (20 U/$\mu$l). I think 10 U are enough for 1 $\mu$g of DNA. For 500 samples, we would end up using 5000 U of each enzyme. It is unfortunate that the chosen enzymes are not the cheapest ones. *SphI-HF*, in particular, comes in tubes of 2500 U for more than €300.00.

Depending on the real size of the genome (between 2.1 and 3.54 pg), we could find 0.37--0.62 pmol of *SphI* ends and 0.47--0.79 pmol
*HindIII* ends in a reaction. Using 6 and 8 pmol of adapters P2 (for *SphI* ends) and P1 (for *HindIII* ends), respectively, there would be a comfortable 10 fold excess of adapters. The 25 nmol scale of synthesis is enough for all samples.

To determine the units of T4 DNA ligase required in the concentration, I should know the concentration of cohesive ends.
Considering both kinds of genomic coehsive ends, and the cohesive ends from the 10-fold excess of adapters, there
could be `r 0.62 + 0.79 + 6.2 + 7.9` pmol of cohesive ends. If the final reaction volume was 50 $\mu$l, the concentration
of cohesive ends would be `r (0.62 + 0.79 + 6.2 + 7.9) / 50` pmol/$\mu$l (or $\mu$M). One unit is the amount of enzyme
required to ligate 50% of 0.12 $\mu$M cohesive ends in 20 $\mu$l of reaction during 30 minutes at 16ºC. The example of
ligase reaction in the NEB website uses 400 units (1 $\mu$l) of ligase to ligate ends at 0.004 $\mu$M, also in 20 $\mu$l.
Let's say I use 800 units (2 $\mu$l at 400000 units/ml) of ligase per reaction, in 50 $\mu$l volumes. That is 5 $\mu$l of 
10X buffer per reaction. In 500 reactions, that adds up to `r 800 * 500` units or 1 ml. That is 4 times the content of
the M0202L product from NEB.

ATP is at 10 mM, and needs to be at 1 mM. 




# References
